import os
import time
import json
import re
import logging
import concurrent.futures
from urllib.parse import urljoin, urlparse
import pandas as pd
import requests
from bs4 import BeautifulSoup

# Selenium imports
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("scraper.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class AquapolisOptimizedScraper:
    def __init__(self, headless=True, max_workers=3):
        self.base_url = "https://aquapolis.ru"
        self.headless = headless
        self.max_workers = max_workers
        self.session = requests.Session()
        self.driver = None
        self.categories = {}
        self.all_products = []
        self.output_dir = 'aquapolis_data'
        
        # Headers mimicking a real browser
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
            'Referer': 'https://aquapolis.ru/'
        }
        self.session.headers.update(self.headers)

    def setup_selenium(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Selenium –¥–ª—è –æ–±—Ö–æ–¥–∞ –∑–∞—â–∏—Ç—ã –∏ –ø–æ–ª—É—á–µ–Ω–∏—è cookies"""
        logger.info("üîß –ó–∞–ø—É—Å–∫ Selenium –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–µ—Å—Å–∏–∏...")
        options = Options()
        if self.headless:
            options.add_argument('--headless=new')
        options.add_argument('--disable-blink-features=AutomationControlled')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument(f'user-agent={self.headers["User-Agent"]}')
        
        try:
            self.driver = webdriver.Chrome(
                service=Service(ChromeDriverManager().install()),
                options=options
            )
            
            # –ú–∞—Å–∫–∏—Ä–æ–≤–∫–∞ webdriver
            self.driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {
                'source': "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"
            })
            
            # –ó–∞—Ö–æ–¥–∏–º –Ω–∞ –≥–ª–∞–≤–Ω—É—é, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å cookies –∏ –ø—Ä–æ–π—Ç–∏ –ø—Ä–æ–≤–µ—Ä–∫–∏
            logger.info("üåç –û—Ç–∫—Ä—ã–≤–∞–µ–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É...")
            self.driver.get(self.base_url)
            time.sleep(5) # –ñ–¥–µ–º –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è Cloudflare/DDOS-GUARD –µ—Å–ª–∏ –µ—Å—Ç—å
            
            # –ü–µ—Ä–µ–Ω–æ—Å–∏–º cookies –≤ requests session
            selenium_cookies = self.driver.get_cookies()
            for cookie in selenium_cookies:
                self.session.cookies.set(cookie['name'], cookie['value'])
            
            logger.info(f"‚úÖ –°–µ—Å—Å–∏—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞. –ü–æ–ª—É—á–µ–Ω–æ {len(selenium_cookies)} cookies.")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ Selenium: {e}")
            raise e

    def get_soup(self, url):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ BeautifulSoup –æ–±—ä–µ–∫—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —á–µ—Ä–µ–∑ requests"""
        try:
            response = self.session.get(url, timeout=15)
            if response.status_code == 200:
                return BeautifulSoup(response.text, 'html.parser')
            else:
                logger.warning(f"‚ö† –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ {url}: Status {response.status_code}")
                return None
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}: {e}")
            return None

    def parse_sitemap(self):
        """–°–±–æ—Ä –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Å –∫–∞—Ä—Ç—ã —Å–∞–π—Ç–∞ –∏–ª–∏ –º–µ–Ω—é"""
        logger.info("üìÇ –°–±–æ—Ä –∫–∞—Ç–µ–≥–æ—Ä–∏–π...")
        
        # –ü–æ–ø—Ä–æ–±—É–µ–º —á–µ—Ä–µ–∑ –∫–∞—Ä—Ç—É —Å–∞–π—Ç–∞
        map_url = f"{self.base_url}/map.html"
        soup = self.get_soup(map_url)
        
        if not soup:
            logger.warning("‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–∞—Ä—Ç—É —Å–∞–π—Ç–∞, –ø—Ä–æ–±—É–µ–º –≥–ª–∞–≤–Ω—É—é...")
            soup = self.get_soup(self.base_url)
            
        if not soup:
            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–æ—Å—Ç—É–ø –∫ —Å–∞–π—Ç—É.")
            return False

        # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        links = soup.find_all('a', href=True)
        count = 0
        
        skip_words = ['login', 'register', 'cart', 'checkout', 'contact', 'about', 'blog', 'news', 'tel:', 'mailto:']
        
        for link in links:
            href = link['href']
            text = link.get_text(strip=True)
            
            if not href.startswith('http'):
                href = urljoin(self.base_url, href)
                
            if self.base_url not in href:
                continue
                
            if any(s in href.lower() for s in skip_words):
                continue
                
            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –ø–æ—Ö–æ–∂–µ–µ –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ç–æ–≤–∞—Ä–æ–≤
            if text and len(text) > 2 and '.html' in href:
                if href not in self.categories.values():
                    self.categories[text] = href
                    count += 1

        logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ {count} –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π.")
        return count > 0

    def parse_product_card(self, card):
        """–ü–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ä—Ç–æ—á–∫–∏ —Ç–æ–≤–∞—Ä–∞ –∏–∑ HTML"""
        try:
            product = {}
            
            # –ù–∞–∑–≤–∞–Ω–∏–µ
            name_tag = card.find(['a', 'div', 'h3', 'h4'], class_=re.compile(r'name|title|header', re.I))
            if not name_tag:
                name_tag = card.find('a')
            
            if name_tag:
                product['name'] = name_tag.get_text(strip=True)
                if name_tag.name == 'a':
                    product['url'] = urljoin(self.base_url, name_tag['href'])
                elif name_tag.find('a'):
                    product['url'] = urljoin(self.base_url, name_tag.find('a')['href'])
            
            if not product.get('name'):
                return None

            # –¶–µ–Ω–∞
            price_tag = card.find(class_=re.compile(r'price|cost|sum', re.I))
            if price_tag:
                price_text = price_tag.get_text(strip=True)
                price_match = re.search(r'(\d[\d\s]*[.,]?\d*)', price_text)
                if price_match:
                    product['price'] = price_match.group(1).replace(' ', '').replace('\xa0', '')
            
            # –ö–∞—Ä—Ç–∏–Ω–∫–∞
            img_tag = card.find('img')
            if img_tag:
                src = img_tag.get('src') or img_tag.get('data-src') or img_tag.get('data-original')
                if src:
                    product['image'] = urljoin(self.base_url, src)
            
            # –ù–∞–ª–∏—á–∏–µ
            stock_tag = card.find(class_=re.compile(r'stock|availability', re.I))
            if stock_tag:
                product['in_stock'] = stock_tag.get_text(strip=True)
            else:
                product['in_stock'] = '–£—Ç–æ—á–Ω—è–π—Ç–µ'

            return product
        except Exception as e:
            return None

    def process_category(self, category_name, category_url):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (–ø–∞–≥–∏–Ω–∞—Ü–∏—è + —Ç–æ–≤–∞—Ä—ã)"""
        logger.info(f"üì¶ –û–±—Ä–∞–±–æ—Ç–∫–∞: {category_name}")
        products = []
        page = 1
        
        while True:
            page_url = f"{category_url}?p={page}" if page > 1 else category_url
            soup = self.get_soup(page_url)
            
            if not soup:
                break
                
            # –ü–æ–∏—Å–∫ –∫–∞—Ä—Ç–æ—á–µ–∫ —Ç–æ–≤–∞—Ä–æ–≤
            product_cards = soup.find_all(class_=re.compile(r'product-item|catalog-item|item-card|products-grid__item', re.I))
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º, –∏—â–µ–º –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ
            if not product_cards:
                potential_cards = soup.find_all('div')
                product_cards = []
                for div in potential_cards:
                    if div.find('img') and div.find(string=re.compile(r'\d+\s*(?:—Ä—É–±|‚ÇΩ)')):
                        product_cards.append(div)
                        
            if not product_cards:
                if page == 1:
                    logger.debug(f"  ‚ö† –ù–µ—Ç —Ç–æ–≤–∞—Ä–æ–≤ –≤ {category_name}")
                break
                
            logger.info(f"  üìÑ –°—Ç—Ä. {page}: –Ω–∞–π–¥–µ–Ω–æ {len(product_cards)} —Ç–æ–≤–∞—Ä–æ–≤")
            
            new_products_count = 0
            for card in product_cards:
                product = self.parse_product_card(card)
                if product and product.get('name'):
                    product['category'] = category_name
                    products.append(product)
                    new_products_count += 1
            
            if new_products_count == 0:
                break
                
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
            next_link = soup.find('a', class_=re.compile(r'next|forward'), href=True)
            pagination = soup.find(class_=re.compile(r'pagination|pager'))
            
            if not next_link and not pagination:
                break
                
            if page > 50:
                break
                
            page += 1
            time.sleep(0.5)
            
        return products

    def run(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –∑–∞–ø—É—Å–∫–∞"""
        start_time = time.time()
        
        try:
            # 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
            self.setup_selenium()
            
            # 2. –°–±–æ—Ä –∫–∞—Ç–µ–≥–æ—Ä–∏–π
            if not self.parse_sitemap():
                logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∏. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ.")
                return
            
            # 3. –ü–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏–π (–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ)
            logger.info(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ {len(self.categories)} –∫–∞—Ç–µ–≥–æ—Ä–∏–π –≤ {self.max_workers} –ø–æ—Ç–æ–∫–∞(–æ–≤)...")
            
            target_categories = list(self.categories.items())
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                future_to_cat = {
                    executor.submit(self.process_category, name, url): name 
                    for name, url in target_categories
                }
                
                for future in concurrent.futures.as_completed(future_to_cat):
                    cat_name = future_to_cat[future]
                    try:
                        cat_products = future.result()
                        if cat_products:
                            self.all_products.extend(cat_products)
                            logger.info(f"  ‚úÖ {cat_name}: —Å–æ–±—Ä–∞–Ω–æ {len(cat_products)} —Ç–æ–≤–∞—Ä–æ–≤")
                    except Exception as e:
                        logger.error(f"  ‚ùå –û—à–∏–±–∫–∞ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ {cat_name}: {e}")

            # 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            self.save_results()
            
        finally:
            if self.driver:
                self.driver.quit()
                
        duration = time.time() - start_time
        logger.info(f"üèÅ –ì–æ—Ç–æ–≤–æ! –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {duration:.2f} —Å–µ–∫. –í—Å–µ–≥–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(self.all_products)}")

    def save_results(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Excel –∏ JSON"""
        if not self.all_products:
            logger.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è.")
            return

        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)

        # Excel
        df = pd.DataFrame(self.all_products)
        excel_path = os.path.join(self.output_dir, 'aquapolis_full.xlsx')
        
        # –£–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏
        cols = ['name', 'price', 'in_stock', 'category', 'url', 'image']
        for c in df.columns:
            if c not in cols:
                cols.append(c)
        
        # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –¥–ª—è –∫—Ä–∞—Å–æ—Ç—ã
        ru_cols = {
            'name': '–ù–∞–∑–≤–∞–Ω–∏–µ',
            'price': '–¶–µ–Ω–∞',
            'in_stock': '–ù–∞–ª–∏—á–∏–µ',
            'category': '–ö–∞—Ç–µ–≥–æ—Ä–∏—è',
            'url': '–°—Å—ã–ª–∫–∞',
            'image': '–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ'
        }
        
        try:
            df = df.reindex(columns=cols)
            df.rename(columns=ru_cols, inplace=True)
            df.to_excel(excel_path, index=False)
            logger.info(f"üíæ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {excel_path}")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è Excel: {e}")
            df.to_csv(os.path.join(self.output_dir, 'aquapolis_dump.csv'), index=False)

if __name__ == "__main__":
    print("="*50)
    print("üöÄ AQUAPOLIS OPTIMIZED SCRAPER")
    print("="*50)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
    try:
        import requests
        import bs4
    except ImportError:
        print("‚ö† –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫...")
        os.system("pip install requests beautifulsoup4 pandas openpyxl selenium webdriver-manager")
        print("‚úÖ –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ —Å–∫—Ä–∏–ø—Ç.")
        exit()

    scraper = AquapolisOptimizedScraper(headless=True, max_workers=5)
    scraper.run()